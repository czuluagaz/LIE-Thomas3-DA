{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Og9SFHPSefpf"
   },
   "source": [
    "# Timeline of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3_aAVkCefpl"
   },
   "source": [
    "## Symbolic NLP (1950s - 1990s)\n",
    "\n",
    "There are two main dates during this period.  \n",
    "\n",
    "In 1950 **[IBM created the first model capable of translating](https://en.wikipedia.org/wiki/Georgetown%E2%80%93IBM_experiment)** 60 sentences from Russian into English. This was the first approach to machine translation. Unfortunately, the algorithm was only able to translate these 60 sentences! As soon as an unknown sentence was presented to the model, it was no longer able to translate it.\n",
    "\n",
    "**[Eliza](https://en.wikipedia.org/wiki/ELIZA)** is a computer program written by Joseph Weizenbaum between 1964 and 1966, which simulates a psychotherapist by reformulating most of the \"patient's\" assertions into questions and asking them. Although Eliza is considered the first chatbot, this model had no level of comprehension, it really just rephrased the questions.\n",
    "\n",
    "Throughout this period, a rules-based symbolic approach was used. Rules-Based uses Linguistic rules and patterns. *E.g English has the structure of SVO (Subject Verb Object), Hindi has SOV (Subject Object Verb)*.  This has been made possible by regular expressions and [context-free grammar](https://en.wikipedia.org/wiki/Context-free_grammar).\n",
    "\n",
    "\n",
    "\n",
    "![](http://wiki.penson.io/images/cfg.png)\n",
    "\n",
    "[source](http://wiki.penson.io/images/cfg.png)\n",
    "\n",
    "The code could look like the part below, for a translation from English to Hindi.\n",
    "\n",
    "```\n",
    "\"have\" :=\n",
    "\n",
    "if \n",
    "  subject(animate)\n",
    "  and object(owned-by-subject)\n",
    "then \n",
    "  translate to \"Kade... aahe\"\n",
    "if \n",
    "  subject(animate)\n",
    "  and object(kinship-with-subject)\n",
    "then\n",
    "  translate to \"laa... aahe\"\n",
    "if \n",
    "  subject(inanimate)\n",
    "then \n",
    "  translate to \"madhye... aahe\"\n",
    "\n",
    "```\n",
    "\n",
    "But this approach has limits. Not only does one have to be an expert in linguistics, but one also has to create a considerable number of rules for moderate effectiveness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMgFPfuZfefS"
   },
   "source": [
    "| Advantages | Disadvantages|\n",
    "|------------|--------------|\n",
    "|No training time.|  Impossible to write all the rules because of the complexity of the languages. |\n",
    "| Quick to execute. | Impossibility to process unknown data. |\n",
    "| Efficient if the inputs are all known. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jkrANchqb3Q"
   },
   "source": [
    "**Exercise :** Explain using your own words (no copy and paste) what a symbolic approach is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQXet-yIqvRS"
   },
   "outputs": [],
   "source": [
    "# create a structure and set of rules using logic, symbols and axiomas to solve a problem\n",
    "# 1. create a structure to store the data\n",
    "# 2. create a set of rules to manipulate the data\n",
    "# 3. create a set of rules to stop the manipulation\n",
    "# 4. create a set of rules to display the results\n",
    "# 5. create a set of rules to repeat the process\n",
    "# 6. create a set of rules to end the process\n",
    "# 7. create a set of rules to save the results\n",
    "# 8. create a set of rules to load the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3hnPUY1efpm"
   },
   "source": [
    "## Statistical NLP (1990s - 2010s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXem55q5WvCN"
   },
   "source": [
    "The idea of the statistical approach is of course to find a model that can make generalities about unknown sentences rather than coding a whole bunch of rules that would be \"hardcoded\". \n",
    " \n",
    "The idea is based on a parallel corpus. It is given as input a sentence in English for example, and the model gives as output the translated sentence. \n",
    "\n",
    "We first tried to do word by word, it didn't work very well, because of the grammatical complexity of some languages. Then we created what we call n-grams. These are groups of words, unigram for one word, bigram for two words, trigram for three words and so on... So instead of doing word by word translations, we do them by groups of words. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFRJjtbAe2Uq"
   },
   "source": [
    "![](https://i.stack.imgur.com/8ARA1.png)\n",
    "\n",
    "[source](https://stackoverflow.com/questions/18193253/what-exactly-is-an-n-gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJ_tcM1i5IPX"
   },
   "source": [
    "During this period, there are 2 models that stand out for NLP : \n",
    "\n",
    "- [Logistic regression](https://en.wikipedia.org/wiki/Support-vector_machine)\n",
    "- [SVM](https://en.wikipedia.org/wiki/Support-vector_machine)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhUtbDcqzSj4"
   },
   "source": [
    "**Exercise :** Load a *sentiment analysis* dataset and create a logistic regression model for classification. (1= positive , 0 =negative). For example, you could use the `twitter_samples` dataset from the `nltk` library.\n",
    "\n",
    "* Tips : Use `CountVectorizer()` and `LogisticRegression()` from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uuAeZqehI79R"
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mtwitter_samples\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('twitter_samples')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/twitter_samples\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\zudel/nltk_data'\n    - 'c:\\\\Users\\\\zudel\\\\projects\\\\LIE-Thomas3-DA\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\zudel\\\\projects\\\\LIE-Thomas3-DA\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\zudel\\\\projects\\\\LIE-Thomas3-DA\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\zudel\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\zudel\\projects\\LIE-Thomas3-DA\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\zudel\\projects\\LIE-Thomas3-DA\\.venv\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mtwitter_samples\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('twitter_samples')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/twitter_samples.zip/twitter_samples/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\zudel/nltk_data'\n    - 'c:\\\\Users\\\\zudel\\\\projects\\\\LIE-Thomas3-DA\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\zudel\\\\projects\\\\LIE-Thomas3-DA\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\zudel\\\\projects\\\\LIE-Thomas3-DA\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\zudel\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# nltk.download('twitter_samples')\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# nltk.download('stopwords')\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# nltk.download('wordnet')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# load the dataset\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m positive_tweets \u001b[38;5;241m=\u001b[39m \u001b[43mtwitter_samples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrings\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive_tweets.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m negative_tweets \u001b[38;5;241m=\u001b[39m twitter_samples\u001b[38;5;241m.\u001b[39mstrings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative_tweets.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# print(positive_tweets[:10])\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# print(negative_tweets[:10])\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# load the stopwords\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zudel\\projects\\LIE-Thomas3-DA\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\zudel\\projects\\LIE-Thomas3-DA\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Users\\zudel\\projects\\LIE-Thomas3-DA\\.venv\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\zudel\\projects\\LIE-Thomas3-DA\\.venv\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mtwitter_samples\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('twitter_samples')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/twitter_samples\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\zudel/nltk_data'\n    - 'c:\\\\Users\\\\zudel\\\\projects\\\\LIE-Thomas3-DA\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\zudel\\\\projects\\\\LIE-Thomas3-DA\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\zudel\\\\projects\\\\LIE-Thomas3-DA\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\zudel\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# MAX 10 LINES Load a *sentiment analysis* dataset and create a logistic regression model for classification. (1= positive , 0 =negative). For example, you could use the `twitter_samples` dataset from the `nltk` library. You should be able to achieve at least 80% accuracy.\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "# nltk.download('twitter_samples')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# load the dataset\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "# print(positive_tweets[:10])\n",
    "# print(negative_tweets[:10])\n",
    "\n",
    "# load the stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# print(stop_words)\n",
    "\n",
    "# load the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "# print(stemmer.stem('working'))\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "# print(tokenizer.tokenize('This is a cooool #dummysmiley: :-) :-P <3'))\n",
    "\n",
    "# clean the tweets\n",
    "def clean_tweets(tweet):\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet) # remove retweet text 'RT'\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet) # remove hyperlinks\n",
    "    tweet = re.sub(r'#', '', tweet) # remove hashtags\n",
    "    tweet = re.sub(r'@[A-Za-z0-9]+', '', tweet) # remove mentions\n",
    "    return tweet\n",
    "\n",
    "# print(clean_tweets(positive_tweets[0]))\n",
    "\n",
    "# tokenize the tweets\n",
    "def tokenize_tweets(tweet):\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    return tweet_tokens\n",
    "\n",
    "# print(tokenize_tweets(positive_tweets[0]))\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(tweet_tokens):\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if word not in stop_words and word not in string.punctuation:\n",
    "            tweets_clean.append(word)\n",
    "    return tweets_clean\n",
    "\n",
    "# print(remove_stopwords(tokenize_tweets(positive_tweets[0])))\n",
    "# print(remove_stopwords(tokenize_tweets(negative_tweets[0])))\n",
    "# print(remove_stopwords(tokenize_tweets(positive_tweets[0])))\n",
    "\n",
    "# stem the words\n",
    "def stem_tweets(tweet_tokens):\n",
    "    tweets_stem = []\n",
    "    for word in tweet_tokens:\n",
    "        stem_word = stemmer.stem(word)\n",
    "        tweets_stem.append(stem_word)\n",
    "    return tweets_stem\n",
    "\n",
    "# print(stem_tweets(remove_stopwords(tokenize_tweets(positive_tweets[0]))))\n",
    "# print(stem_tweets(remove_stopwords(tokenize_tweets(negative_tweets[0]))))\n",
    "# print(stem_tweets(remove_stopwords(tokenize_tweets(positive_tweets[0]))))\n",
    "\n",
    "# clean, tokenize, remove stopwords and stem the tweets\n",
    "def process_tweets(tweet):\n",
    "    tweet = clean_tweets(tweet)\n",
    "    tweet_tokens = tokenize_tweets(tweet)\n",
    "    tweet_tokens = remove_stopwords(tweet_tokens)\n",
    "    tweet_tokens = stem_tweets(tweet_tokens)\n",
    "    return tweet_tokens\n",
    "\n",
    "# print(process_tweets(positive_tweets[0]))\n",
    "# print(process_tweets(negative_tweets[0]))\n",
    "# print(process_tweets(positive_tweets[0]))\n",
    "\n",
    "# create a list of tuples containing the tweet and the label\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "# print(get_tweets_for_model(process_tweets(positive_tweets[0])))\n",
    "# print(get_tweets_for_model(process_tweets(negative_tweets[0])))\n",
    "# print(get_tweets_for_model(process_tweets(positive_tweets[0])))\n",
    "# print(get_tweets_for_model(process_tweets(negative_tweets[0])))\n",
    "# print(get_tweets_for_model(process_tweets(positive_tweets[0])))\n",
    "# print(get_tweets_for_model(process_tweets(negative_tweets[0])))\n",
    "# print(get_tweets_for_model(process_tweets(positive_tweets[0])))\n",
    "        \n",
    "# create the dataset\n",
    "positive_tweets_set = []\n",
    "negative_tweets_set = []\n",
    "for tweet in positive_tweets:\n",
    "    positive_tweets_set.append((get_tweets_for_model(process_tweets(tweet)), 'Positive'))\n",
    "for tweet in negative_tweets:\n",
    "    negative_tweets_set.append((get_tweets_for_model(process_tweets(tweet)), 'Negative'))\n",
    "# print(positive_tweets_set[:10])\n",
    "# print(negative_tweets_set[:10])\n",
    "    \n",
    "# split the dataset into training and testing\n",
    "dataset = positive_tweets_set + negative_tweets_set\n",
    "random.shuffle(dataset)\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]\n",
    "# print(len(train_data))\n",
    "# print(len(test_data))\n",
    "\n",
    "# create the model\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "# print(classifier.classify(dict([token, True] for token in process_tweets('This is a good book'))))\n",
    "# print(classifier.classify(dict([token, True] for token in process_tweets('This is a bad book'))))\n",
    "\n",
    "# test the model\n",
    "accuracy = classify.accuracy(classifier, test_data)\n",
    "# print(accuracy)\n",
    "\n",
    "# save the model\n",
    "# f = open('sentiment_analysis_model.pickle', 'wb')\n",
    "# pickle.dump(classifier, f)\n",
    "# f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctoAelW8JpQY"
   },
   "source": [
    "OK, it works well! As you can see, the computer needs to translate the words into a vector to understand a sentence. But it's not that simple. In fact, for a computer, it is a very complex task. Indeed, the problem is quite simple.\n",
    "\n",
    "When you teach a model to recognize a cat, for example, the model actually learns to create a vector which is a universal representation of the cat. This is possible if you show him enough examples of cats !\n",
    "\n",
    "But for language, it is more complicated!\n",
    "\n",
    "There are so many ways to express oneself, so many subtleties in our languages, that it becomes very difficult for a computer to really understand the deep meaning of a sentence.\n",
    "\n",
    "Let's look at this sentence :\n",
    "\n",
    "<center><b>I do not recommend this product which is bad.</b></center>\n",
    "and : \n",
    "<center><b>I do recommend this product which is not bad.</b></center>  \n",
    "\n",
    "\n",
    "These two sentences contain the same words, but their meanings are different.\n",
    "\n",
    "The machine learning model will not be able to tell the difference between these two sentences.  With this model we lose an important piece of information, which is temporal information. Indeed, here, the order of the words has no effect on the prediction made by the model. However, as we have seen, this information can change the deep meaning of the sentence. \n",
    "\n",
    "Another problem, if we have a very large dataset, the computation time for training the model could be very long.\n",
    "\n",
    "For these reasons, we have started to study possible alternatives. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wA6Uw1OmhRBM"
   },
   "source": [
    "| Advantages | Disadvantages|\n",
    "|------------|--------------|\n",
    "| Can make a probalility when it receives an unknown input.|  The calculation time is long.  |\n",
    "| Allows to make generalities in order to avoid coding rules. | Loss of information (Word order)  |\n",
    "| | No context |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFKmUpyxefpm"
   },
   "source": [
    "## Neural NLP (2010s - present)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNZJY6wRpWB7"
   },
   "source": [
    "### Word2Vec  (2010 - 2014)\n",
    "\n",
    "Word2Vec is a module released in the 2010's. It's one of the first to use neural networks to make word representations. It's based on 2 architectures, CBOW and Skip-Gram. In both cases, they are 2-layer neural networks.\n",
    "\n",
    "CBOW uses surrounding words to predict who is in the middle. Skip-gram is used to understand the context of the sentence. It thus makes it possible, among other things, to make classification. \n",
    "\n",
    "![word2vec](https://miro.medium.com/max/2400/1*cuOmGT7NevP9oJFJfVpRKA.png)\n",
    "\n",
    "But we still have a problem. Certainly the model formation will be faster and more efficient, but we still don't have time information. He can recognize that the word \"apple\" is close to the word \"pear\". That's good enough! But the word order is still not taken into account in our model. He won't be able to differentiate the 2 sentences.\n",
    "\n",
    "<center><b>I do not recommend this product which is bad.</b></center>\n",
    "and : \n",
    "<center><b>I do recommend this product which is not bad.</b></center>  \n",
    "\n",
    "To try to bring a solution to this problem, we tried to work with recurrent neural networks. \n",
    "\n",
    "| Advantages | Disadvantages|\n",
    "|------------|--------------|\n",
    "| Can output a probability when it receives an unknown input.|  The calculation time is long.  |\n",
    "| Allows to make generalities in order to avoid coding rules. | Loss of information (Word order)  |\n",
    "| Can know the similarity between two words. (ex: 'Car, Motorcycle') | |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QexsVFH2sIWE"
   },
   "source": [
    "**Exercise :** Explain using your own words (no copy and paste) what a skip-gram is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQmI6XMuZZCo"
   },
   "source": [
    "### RNN/LSTM (2014 - 2017)\n",
    "\n",
    "Recursive neural networks are similar to \"classical\" neural networks, but they differ from them in that they use feedback loops to process a sequence of data that shapes the final result. The end result may itself be a sequence of data. These feedback loops allow information to persist, an effect often equated with memory.\n",
    "\n",
    "All inputs are connected to each other and feed information back into the network. Put simply, the previous weights of a word can be changed by the following words. And the following words depend on the weights of the previous words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2G0GXXKefpo"
   },
   "source": [
    "So if we take the phrase \"what time is it?\" the vector of the word \"time\" will contain information about the previous word \"what\". Likewise, the vector of the word \"is\" will contain information of \"What\" and \"time\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWuzcbsDefpp"
   },
   "source": [
    "![gif_rnn](https://miro.medium.com/max/500/1*1U8H9EZiDqfylJU7Im23Ag.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAzHDVtJefpp"
   },
   "source": [
    "The big difference between classical neural networks and recursive neural networks is that those of RNN can take one or more input vectors and produce one or more output vectors. The output(s) are influenced not only by weights applied to the inputs like a regular NN, but also by a \"hidden\" state vector representing the context based on the previous inputs/outputs. This implies that regular NN must have input and output vectors that have fixed sizes, whereas with RNNs, the input and output must not.\n",
    "\n",
    "\n",
    "The relationships between the vectors can be represented as follows: \n",
    "\n",
    "![rnn_schema](https://i.stack.imgur.com/b4sus.jpg)\n",
    "[source](https://stackoverflow.com/questions/43034960/many-to-one-and-many-to-many-lstm-examples-in-keras)\n",
    "\n",
    "The red rectangles represent the input vectors. The blue rectangles represent the output vectors. The green rectangles are the state vectors.\n",
    "\n",
    "* **one to one :** Representation of a traditional **non-recurrent** Neural Network\n",
    "* **one to many :** A fixed vector as input and vector sequences as output. (Example an image as input and a description of the image as output.)\n",
    "* **many to one :** Sequences of vectors as inputs, and one vector as output. (Example: Sentence classification)\n",
    "* **many to many :** Vector sequences as inputs and vector sequences as outputs. (Sentence translation and/or Name entity recognition.)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPSqemyxcEug"
   },
   "source": [
    "But RNNs have a problem. For one thing, this architecture has a short-term memory. This implies that while state vectors can contain information about neighboring words, this information is limited by distance. This works very well on small sequences (for example for the next 3 or 4 words in a sentence). But if the sequences are long, the gradients (values calculated to tune the network) computed during their training (backpropagation) either vanish (multiplication of many 0 < values < 1) or explode (multiplication of many large values) causing it to train very slowly.\n",
    "\n",
    "To model very long term dependencies, it is necessary to give recurrent neural networks the ability to maintain a state over a long period of time.\n",
    "\n",
    "This is where LSTM (Long Short Term Memory) networks come in. These networks have an internal memory called cell. The cell allows to maintain a state as long as necessary. This cell consists of a numerical value that the network can control according to the situation. This cell can have three control gates, which are activation functions. There is an input gate that decides whether the input should change the content of the cell. There is also a forget gate that decides whether to reset the content of the cell to 0. And finally there is an output gate that decides if the content of the cell should influence the output of the neuron.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Mohamed_Akram_Zaytar/publication/304066008/figure/fig7/AS:494978687746048@1495023523357/A-simple-LSTM-gate-with-only-input-output-and-forget-gates.png)\n",
    "\n",
    "[source](https://www.researchgate.net/figure/A-simple-LSTM-gate-with-only-input-output-and-forget-gates_fig7_304066008)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPTLsbuOmRVx"
   },
   "source": [
    "These operations in the gates allow the LSTM to retain or delete information that it has in memory. For example, in our sentence \"Last night I ate a hamburger and some\", it is important to retain the words \"hamburger\" and \"eat\" while the determiners \"a\", \"and\" can be forgotten by the network.\n",
    "\n",
    "The data stored in the memory of the network is in fact a vector noted $c_t$ : the state of the cell. As this state depends on the previous state $c_{t-1}$, which itself depends on still previous states, the network can keep information that it has seen a long time before (contrary to the classical RNN).\n",
    "\n",
    "\n",
    "[More resources](http://www.diva-portal.org/smash/get/diva2:1216739/FULLTEXT01.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yea12uEQpnuY"
   },
   "source": [
    "| Advantages | Disadvantages|\n",
    "|------------|--------------|\n",
    "| They are able to model long-term sequence dependencies|  They increase the computing complexity compared to the RNN with the introduction of more parameters to learn. |\n",
    "| They are more robust to the problem of short memory than ‘Vanilla’ RNNs since the definition of the internal memory is changed   | The memory required is higher than the one of ‘Vanilla’ RNNs due to the presence of several memory cells. | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz5sTtTEq4vj"
   },
   "source": [
    "**Exercise :** Explain using your own words, (no copy and paste) how RNN works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SEPPRNurk68"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eviN5d8HtK_Q"
   },
   "source": [
    "### Transformer (2017 - present)\n",
    "\n",
    "Natural language processing is, today, essentially dominated by sequence-to-sequence (or seq2seq) models. A seq2seq model is a model that takes a sequence (a sequence of elements of the same type) as input and returns a sequence as output. The example par excellence for this type of model is the translation of text. Among the seq2seq models that have emerged in the last few years, if there is one that stands out, it is the Transformer. The Transformer is a sequence-to-sequence model based on the attention mechanism and not on a recurrent neural network as it was the case for the previous models. On the other hand, we will keep the sequences as inputs and outputs. \n",
    "\n",
    "\n",
    "The Attention mechanism is a measure of how well two elements in two sequences are related. In a sequence-to-sequence context in NLP, the self-attention mechanism is used to determine which word or sequence of words in the entire sequence gives context elements when processing a word. It thus makes it possible to capture the relationships between words, even if they are far apart from each other in the sequence.\n",
    "\n",
    "![](https://i.imgur.com/PHWQnbX.png)  \n",
    "[source](https://www.kaggle.com/residentmario/transformer-architecture-self-attention)\n",
    "\n",
    "* [more resources](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "\n",
    "A transformer consists of two parts, an encoder and a decoder. The encoder is a neural network used to transform the input sequence into a vector representation of the sequence.  The head of attention mechanism then captures for each word the context elements relevant to it and integrates it into the vector generated by the encoder. This step is repeated several times simultaneously for all the words, thus parallelizing the process. The final vector representation thus generated by the encoder then serves as input to a second network, the decoder, which is used to generate words sequentially.\n",
    "\n",
    "![encode-decoder](https://miro.medium.com/max/1284/1*1BFAQXkNiLySIhB__24EkQ.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tULFn5KQaFAM"
   },
   "source": [
    "There are several models using the transformer architecture, such as Bert, XLnet, or GPT-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ka-9MDaprt-U"
   },
   "source": [
    "**Exercise :** Explain using your own words (no copy and paste) what a layer of attention is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKhcMkL5rrm6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CK9l-2ggefpr"
   },
   "source": [
    "## In conclusion\n",
    "\n",
    "For many NLP problems, a statistical language model is required. Models based on neural networks offer the best results, thanks in particular to their generalization capability. As for the transformers type models that have recently appeared, they allow to reach very good performances on some NLP tasks with limited data.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "timeline_of_nlp.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
